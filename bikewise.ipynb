{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371d66c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import getpass\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession, catalog\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd7ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the timestamp to pass as parameter to the API-request\n",
    "\n",
    "def get_timestamp(queue):\n",
    "    current_ts = int(time.time())\n",
    "    today = datetime.date.today()\n",
    "    today = today.strftime(\"%Y-%m-%d\")\n",
    "    to_time = datetime.datetime.strptime(today,\"%Y-%m-%d\")  \n",
    "    today_ts = int(datetime.datetime.timestamp(to_time)) - 14400\n",
    "    queue.put([today_ts, current_ts, today])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd68ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_local(timestamps):\n",
    "    os.popen(f'mkdir -p  ~/shubham/bike_data/{timestamps[2]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00d87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the response from the API and write to file\n",
    "\n",
    "def get_response(url, timestamps, queue):\n",
    "    \n",
    "    header = {\n",
    "      \"Cache-Control\": \"max-age=0, private, must-revalidate\",\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    parameters = { \"page\": 1,\n",
    "                   \"per_page\": 10000,\n",
    "                   \"occurred_before\": timestamps[1],\n",
    "                   \"occurred_after\": timestamps[0]\n",
    "              }\n",
    "    try:\n",
    "        response = requests.get(url, headers=header, params = parameters)\n",
    "        data = response.json()\n",
    "        data = data[\"incidents\"]\n",
    "        queue.put(data)\n",
    "    except Exception as e:\n",
    "        logging.info(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e317cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(data, timestamps):\n",
    "    for values in data:\n",
    "        with open(f'bike_data/{timestamps[2]}/{timestamps[2]}.json', 'a') as f:\n",
    "            f.write(json.dumps(values) + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "712fd410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_session(username, queue):\n",
    "    spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port', '0'). \\\n",
    "        config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Bikewise'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()\n",
    "    spark.conf.set('spark.sql.shuffle.partition', 10)\n",
    "    queue.put(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cea933b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_hdfs(timestamps, hdfs_username):\n",
    "    os.popen(f'hdfs dfs -mkdir -p /user/{hdfs_username}/bikewise/raw')\n",
    "    os.popen(f'hdfs dfs -mkdir -p /user/{hdfs_username}/bikewise/initial')\n",
    "    os.popen(f'hdfs dfs -mkdir -p /user/{hdfs_username}/bikewise/final')\n",
    "    os.popen(f'hdfs dfs -mkdir -p /user/{hdfs_username}/bikewise/final/incident_reports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c0a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(spark):\n",
    "    spark.sql(f'create database if not exists {username}_bikewise_raw')\n",
    "    spark.sql(f'create database if not exists {username}_bikewise_initial')\n",
    "    spark.sql(f'create database if not exists {username}_bikewise_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96354335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(timestamps, hdfs_username, queue):\n",
    "    os.popen(f'hdfs dfs -copyFromLocal ~/shubham/bike_data/{timestamps[2]} /user/{hdfs_username}/bikewise/raw')\n",
    "    queue.put(f'/user/{hdfs_username}/bikewise/raw/{timestamps[2]}/{timestamps[2]}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a7814e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def copy_report(timestamps, hdfs_username, queue):\n",
    "#     os.popen(f'''hdfs dfs -copyFromLocal ~/shubham/bike_data/report_{timestamps[2]}.pdf \n",
    "#              /user/{hdfs_username}/bikewise/final/incident_reports''')\n",
    "#     queue.put(f'/user/{hdfs_username}/bikewise/final/incident_reports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cf624c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_thread_directory(timestamps, hdfs_username):\n",
    "    stream = os.popen(f'hdfs dfs -ls /user/{hdfs_username}/bikewise/')\n",
    "    output = stream.readlines()\n",
    "    for ln in output:\n",
    "        print(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8dbd80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_thread_warehouse(timestamps, hdfs_username):\n",
    "    stream = os.popen(f'hdfs dfs -ls /user/{hdfs_username}/warehouse/')\n",
    "    output = stream.readlines()\n",
    "    for ln in output:\n",
    "        print(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d34b33c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe(hdfs_username, timestamps, spark, file_path, queue):\n",
    "    df_raw = spark. \\\n",
    "    read. \\\n",
    "    json(file_path)\n",
    "\n",
    "    df_raw = df_raw. \\\n",
    "    withColumn('year', date_format(current_date(), 'yyyy')). \\\n",
    "    withColumn('month', date_format(current_date(), 'MM')). \\\n",
    "    withColumn('day', date_format(current_date(), 'dd'))\n",
    "    \n",
    "    queue.put(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6245ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json(timestamps, hdfs_username, df, table_name):\n",
    "    df.write.format('json').save(f'/user/{hdfs_username}/bikewise/{table_name}/{timestamps[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7b427f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_table(spark, hdfs_username, df, table_name):\n",
    "    tables = spark.catalog.listTables(f'{hdfs_username}_bikewise_{table_name}')\n",
    "    found = False\n",
    "    for table in tables:\n",
    "        if(list(table)[0] == f'incidents_{table_name}'):\n",
    "            found = True\n",
    "            break\n",
    "    if(found):\n",
    "        df. \\\n",
    "        write. \\\n",
    "        mode('append'). \\\n",
    "        partitionBy('year', 'month', 'day'). \\\n",
    "        parquet(f'/user/{hdfs_username}/warehouse/{hdfs_username}_bikewise_{table_name}.db/incidents_{table_name}')\n",
    "\n",
    "        spark.sql(f'''MSCK REPAIR TABLE \n",
    "              {hdfs_username}_bikewise_{table_name}.incidents_{table_name}''')\n",
    "    else:\n",
    "        df. \\\n",
    "        write. \\\n",
    "        partitionBy('year', 'month', 'day'). \\\n",
    "        saveAsTable(f'{hdfs_username}_bikewise_{table_name}.incidents_{table_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f9d9a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(timestamps, df):\n",
    "    df = df.select('type'). \\\n",
    "        groupBy(col('type')).count()\n",
    "    \n",
    "    df = df.toPandas()\n",
    "    \n",
    "    graph = plt.figure(figsize=(10, 8))\n",
    "    splot=sns.barplot(x=\"type\",y=\"count\",data = df)\n",
    "    for p in splot.patches:\n",
    "        splot.annotate(format(p.get_height(), '.0f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, 9), \n",
    "                   textcoords = 'offset points')\n",
    "    plt.xlabel(\"Incident Type\", size=14)\n",
    "    plt.ylabel(\"Count\", size=14)\n",
    "    plt.title(\"Count of Incidents\", size = 20)\n",
    "    plt.savefig(f'bike_data/reports/report_{timestamps[2]}.pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70e2e9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Timestamps calulated\n",
      "INFO:root:Local directory created\n",
      "INFO:root:Received response from the API\n",
      "INFO:root:File for received data created\n",
      "INFO:root:Spark session created\n",
      "INFO:root:HDFS directories created\n",
      "INFO:root:Database creation validated\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/logging/__init__.py\", line 994, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/logging/__init__.py\", line 840, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/logging/__init__.py\", line 577, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/logging/__init__.py\", line 338, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 619, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/asyncio/base_events.py\", line 442, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/asyncio/base_events.py\", line 1462, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 814, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 775, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 374, in dispatch_queue\n",
      "    yield self.process_one()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 250, in wrapper\n",
      "    runner = Runner(ctx_run, result, future, yielded)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 741, in __init__\n",
      "    self.ctx_run(self.run)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 775, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 538, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2867, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-6004950f0f9d>\", line 52, in <module>\n",
      "    logging.info(\"File copied to HDFS at the location: \", file_path)\n",
      "Message: 'File copied to HDFS at the location: '\n",
      "Arguments: ('/user/itv000579/bikewise/raw/2021-07-01/2021-07-01.json',)\n",
      "INFO:root:Checking HDFS-user warehouse directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-06-22 16:01 /user/itv000579/warehouse/itv000579_airtraffic.db\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-07-01 06:45 /user/itv000579/warehouse/itv000579_bikewise_final.db\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-07-01 06:45 /user/itv000579/warehouse/itv000579_bikewise_initial.db\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-07-01 06:45 /user/itv000579/warehouse/itv000579_bikewise_raw.db\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-06-26 05:44 /user/itv000579/warehouse/itv000579_demo.db\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-06-28 03:10 /user/itv000579/warehouse/itv000579_hr.db\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-06-22 15:56 /user/itv000579/warehouse/itv000579_hr_db.db\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-06-26 00:49 /user/itv000579/warehouse/itv000579_nyse.db\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-06-27 15:40 /user/itv000579/warehouse/itv000579_retail.db\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Checking HDFS-user file directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-07-01 06:45 /user/itv000579/bikewise/final\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-07-01 06:45 /user/itv000579/bikewise/initial\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-07-01 06:45 /user/itv000579/bikewise/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Dataframe created\n",
      "INFO:root:Data inserted into Raw-Table\n",
      "INFO:root:Processed JSON file saved in intermediate storage\n",
      "INFO:root:Data saved and inserted into Intermediate-DB \n",
      "INFO:root:Processed JSON file saved in final storage\n",
      "INFO:root:Data saved and inserted into Final-DB \n",
      "INFO:root:Daily-Report created and saved locally\n",
      "INFO:root:Process completed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    url = f'https://bikewise.org:443/api/v2/incidents'\n",
    "#     url = f'https://bikewise.org:443/api/v2/inci'\n",
    "\n",
    "    hdfs_username = 'itv000579'\n",
    "    username = getpass.getuser()\n",
    "    queue = queue.Queue()\n",
    "    \n",
    "    t1 = threading.Thread(target = get_timestamp, args = (queue,))\n",
    "    t1.start()\n",
    "    t1.join()\n",
    "    timestamps = queue.get()\n",
    "    logging.info(\"Timestamps calulated\")\n",
    "    \n",
    "    t2 = threading.Thread(target = mkdir_local, args = (timestamps,))\n",
    "    t2.start()\n",
    "    t2.join()\n",
    "    logging.info(\"Local directory created\")\n",
    "    \n",
    "    t3 = threading.Thread(target = get_response, args = (url, timestamps, queue))\n",
    "    t3.start()\n",
    "    t3.join()\n",
    "    data = queue.get()\n",
    "    logging.info(\"Received response from the API\")\n",
    "    \n",
    "    t4 = threading.Thread(target = create_file, args = (data, timestamps))\n",
    "    t4.start()\n",
    "    t4.join()\n",
    "    logging.info(\"File for received data created\")\n",
    "    \n",
    "    t5 = threading.Thread(target = spark_session, args = (username, queue))\n",
    "    t5.start()\n",
    "    t5.join()\n",
    "    spark = queue.get()\n",
    "    logging.info(\"Spark session created\")\n",
    "    \n",
    "    t6 = threading.Thread(target = mkdir_hdfs, args = (timestamps, hdfs_username))\n",
    "    t6.start()\n",
    "    t6.join()\n",
    "    logging.info(\"HDFS directories created\")\n",
    "    \n",
    "    t7 = threading.Thread(target = create_db, args = (spark,))\n",
    "    t7.start()\n",
    "    t7.join()\n",
    "    logging.info(\"Database creation validated\")\n",
    "    \n",
    "    t8 = threading.Thread(target = copy_files, args = (timestamps, hdfs_username, queue))\n",
    "    t8.start()\n",
    "    t8.join()\n",
    "    file_path = queue.get()\n",
    "    logging.info(\"File copied to HDFS at the location: \", file_path)\n",
    "    \n",
    "    t9 = threading.Thread(target = dummy_thread_warehouse, args = (timestamps, hdfs_username))\n",
    "    t9.start()\n",
    "    t9.join()\n",
    "    logging.info(\"Checking HDFS-user warehouse directory\")\n",
    "    \n",
    "    t10 = threading.Thread(target = dummy_thread_directory, args = (timestamps, hdfs_username))\n",
    "    t10.start()\n",
    "    t10.join()\n",
    "    logging.info(\"Checking HDFS-user file directory\")\n",
    "    \n",
    "    t11 = threading.Thread(target = dataframe, args = (hdfs_username, timestamps, spark, file_path, queue))\n",
    "    t11.start()\n",
    "    t11.join()\n",
    "    df = queue.get()\n",
    "    logging.info(\"Dataframe created\")\n",
    "    \n",
    "    t12 = threading.Thread(target = insert_into_table, args = (spark, hdfs_username, df, 'raw'))\n",
    "    t12.start()\n",
    "    t12.join()\n",
    "    logging.info(\"Data inserted into Raw-Table\")\n",
    "    \n",
    "    df_init = df. \\\n",
    "    select('id', 'type','title', 'description', 'location_type',\n",
    "       'location_description', 'media.image_url', 'occurred_at','updated_at', \n",
    "        'type_properties', 'year', 'month', 'day')\n",
    "    \n",
    "    t13 = threading.Thread(target = df_to_json, args = (timestamps, hdfs_username, df_init, 'initial'))\n",
    "    t13.start()\n",
    "    t13.join()\n",
    "    logging.info('Processed JSON file saved in intermediate storage')\n",
    "    \n",
    "    t14 = threading.Thread(target = insert_into_table, args = (spark, hdfs_username, df_init, 'initial'))\n",
    "    t14.start()\n",
    "    t14.join()\n",
    "    logging.info(\"Data saved and inserted into Intermediate-DB \")\n",
    "    \n",
    "    df_final = df_init. \\\n",
    "    select('id', 'type','title', 'description',\n",
    "       'location_description', 'image_url', \n",
    "       'occurred_at','updated_at', 'year', 'month', 'day'). \\\n",
    "    withColumn('occurred_at', from_unixtime('occurred_at', \"yyyy-MM-dd HH:mm:ss\")). \\\n",
    "    withColumn('updated_at', from_unixtime('updated_at', \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    \n",
    "    \n",
    "    t15 = threading.Thread(target = df_to_json, args = (timestamps, hdfs_username, df_final, 'final'))\n",
    "    t15.start()\n",
    "    t15.join()\n",
    "    logging.info('Processed JSON file saved in final storage')\n",
    "    \n",
    "    \n",
    "    t16 = threading.Thread(target = insert_into_table, args = (spark, hdfs_username, df_final, 'final'))\n",
    "    t16.start()\n",
    "    t16.join()\n",
    "    logging.info(\"Data saved and inserted into Final-DB \")\n",
    "    \n",
    "    t17 = threading.Thread(target = create_report, args = (timestamps, df_final))\n",
    "    t17.start()\n",
    "    t17.join()\n",
    "    logging.info(\"Daily-Report created and saved locally\")\n",
    "    \n",
    "    \n",
    "#     t18 = threading.Thread(target = copy_report, args = (timestamps, hdfs_username, queue))\n",
    "#     t18.start()\n",
    "#     t18.join()\n",
    "#     file_location = queue.get()\n",
    "#     logging.info(\"Report copied to HDFS at the location: \", file_location)\n",
    "    \n",
    "    logging.info(\"Process completed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169855f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

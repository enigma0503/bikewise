{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f68e7c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import getpass\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession, catalog\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1fca1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the timestamp to pass as parameter to the API-request\n",
    "\n",
    "def get_timestamp(queue):\n",
    "    current_ts = int(time.time())\n",
    "    today = datetime.date.today()\n",
    "    today = today.strftime(\"%Y-%m-%d\")\n",
    "    to_time = datetime.datetime.strptime(today,\"%Y-%m-%d\")  \n",
    "    today_ts = int(datetime.datetime.timestamp(to_time)) - 14400\n",
    "    queue.put([today_ts, current_ts, today])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8dbfa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_local(timestamps):\n",
    "    os.popen(f'mkdir -p  ~/shubham/bike_data/{timestamps[2]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9ef978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the response from the API and write to file\n",
    "\n",
    "def get_response(url, timestamps, queue):\n",
    "    \n",
    "    header = {\n",
    "      \"Cache-Control\": \"max-age=0, private, must-revalidate\",\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    parameters = { \"page\": 1,\n",
    "                   \"per_page\": 10000,\n",
    "                   \"occurred_before\": timestamps[1],\n",
    "                   \"occurred_after\": timestamps[0]\n",
    "              }\n",
    "    try:\n",
    "        response = requests.get(url, headers=header, params = parameters)\n",
    "        data = response.json()\n",
    "        data = data[\"incidents\"]\n",
    "        queue.put(data)\n",
    "    except Exception as e:\n",
    "        logging.info(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abdda087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(data, timestamps):\n",
    "    for values in data:\n",
    "        with open(f'bike_data/{timestamps[2]}/{timestamps[2]}.json', 'a') as f:\n",
    "            f.write(json.dumps(values) + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26eb98f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_session(username, queue):\n",
    "    spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port', '0'). \\\n",
    "        config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Bikewise'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()\n",
    "    spark.conf.set('spark.sql.shuffle.partition', 10)\n",
    "    queue.put(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fb3069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_hdfs(timestamps, hdfs_username):\n",
    "    os.popen(f'hdfs dfs -mkdir -p /user/{hdfs_username}/bikewise/raw')\n",
    "    os.popen(f'hdfs dfs -mkdir -p /user/{hdfs_username}/bikewise/initial')\n",
    "    os.popen(f'hdfs dfs -mkdir -p /user/{hdfs_username}/bikewise/final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "556aa5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(spark):\n",
    "    spark.sql(f'create database if not exists {username}_bikewise_raw')\n",
    "    spark.sql(f'create database if not exists {username}_bikewise_initial')\n",
    "    spark.sql(f'create database if not exists {username}_bikewise_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd5b3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(timestamps, hdfs_username, queue):\n",
    "    os.popen(f'hdfs dfs -copyFromLocal ~/shubham/bike_data/{timestamps[2]} /user/{hdfs_username}/bikewise/raw')\n",
    "    queue.put(f'/user/{hdfs_username}/bikewise/raw/{timestamps[2]}/{timestamps[2]}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7238b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_thread(timestamps, hdfs_username):\n",
    "    stream = os.popen(f'hdfs dfs -ls -R /user/{hdfs_username}/bikewise/')\n",
    "    output = stream.readlines()\n",
    "    for ln in output:\n",
    "        print(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a349ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe(hdfs_username, timestamps, spark, file_path, queue):\n",
    "    df_raw = spark. \\\n",
    "    read. \\\n",
    "    json(file_path)\n",
    "\n",
    "    df_raw = df_raw. \\\n",
    "    withColumn('year', date_format(current_date(), 'yyyy')). \\\n",
    "    withColumn('month', date_format(current_date(), 'MM')). \\\n",
    "    withColumn('day', date_format(current_date(), 'dd'))\n",
    "    \n",
    "    queue.put(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfe6beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json(timestamps, hdfs_username, df, table_name):\n",
    "    df.write.format('json').save(f'/user/{hdfs_username}/bikewise/{table_name}/{timestamps[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be16134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_table(spark, hdfs_username, df, table_name):\n",
    "    tables = spark.catalog.listTables(f'{hdfs_username}_bikewise_{table_name}')\n",
    "    found = False\n",
    "    for table in tables:\n",
    "        if(list(table)[0] == f'incidents_{table_name}'):\n",
    "            found = True\n",
    "            break\n",
    "    if(found):\n",
    "        df. \\\n",
    "        write. \\\n",
    "        mode('append'). \\\n",
    "        partitionBy('year', 'month', 'day'). \\\n",
    "        parquet(f'/user/{hdfs_username}/warehouse/{hdfs_username}_bikewise_{table_name}.db/incidents_{table_name}')\n",
    "\n",
    "        spark.sql(f'''MSCK REPAIR TABLE \n",
    "              {hdfs_username}_bikewise_{table_name}.incidents_{table_name}''')\n",
    "    else:\n",
    "        df. \\\n",
    "        write. \\\n",
    "        partitionBy('year', 'month', 'day'). \\\n",
    "        saveAsTable(f'{hdfs_username}_bikewise_{table_name}.incidents_{table_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efefa8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Timestamps calulated\n",
      "INFO:root:Local directory created\n",
      "INFO:root:Received response from the API\n",
      "INFO:root:File for received data created\n",
      "INFO:root:Spark session created\n",
      "INFO:root:HDFS directories created\n",
      "INFO:root:Database creation validated\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/logging/__init__.py\", line 994, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/logging/__init__.py\", line 840, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/logging/__init__.py\", line 577, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/logging/__init__.py\", line 338, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 619, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/asyncio/base_events.py\", line 442, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/asyncio/base_events.py\", line 1462, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 814, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 775, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 538, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n",
      "    return f(*args, **kw)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2867, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-15-df1ad04badeb>\", line 52, in <module>\n",
      "    logging.info(\"File copied to HDFS at the location: \", file_path)\n",
      "Message: 'File copied to HDFS at the location: '\n",
      "Arguments: ('/user/itv000579/bikewise/raw/2021-07-01/2021-07-01.json',)\n",
      "INFO:root:Checking HDFS-user file directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - itv000579 supergroup          0 2021-07-01 01:57 /user/itv000579/bikewise/final\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-07-01 01:57 /user/itv000579/bikewise/initial\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-07-01 01:57 /user/itv000579/bikewise/raw\n",
      "\n",
      "drwxr-xr-x   - itv000579 supergroup          0 2021-07-01 01:57 /user/itv000579/bikewise/raw/2021-07-01\n",
      "\n",
      "-rw-r--r--   3 itv000579 supergroup          0 2021-07-01 01:57 /user/itv000579/bikewise/raw/2021-07-01/2021-07-01.json._COPYING_\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Dataframe created\n",
      "INFO:root:Data inserted into Raw-Table\n",
      "INFO:root:Processed JSON file saved\n",
      "INFO:root:Data saved and inserted into Intermediate-DB \n",
      "INFO:root:Process completed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    url = f'https://bikewise.org:443/api/v2/incidents'\n",
    "#     url = f'https://bikewise.org:443/api/v2/inci'\n",
    "\n",
    "    hdfs_username = 'itv000579'\n",
    "    username = getpass.getuser()\n",
    "    queue = queue.Queue()\n",
    "    \n",
    "    t1 = threading.Thread(target = get_timestamp, args = (queue,))\n",
    "    t1.start()\n",
    "    t1.join()\n",
    "    timestamps = queue.get()\n",
    "    logging.info(\"Timestamps calulated\")\n",
    "    \n",
    "    t2 = threading.Thread(target = mkdir_local, args = (timestamps,))\n",
    "    t2.start()\n",
    "    t2.join()\n",
    "    logging.info(\"Local directory created\")\n",
    "    \n",
    "    t3 = threading.Thread(target = get_response, args = (url, timestamps, queue))\n",
    "    t3.start()\n",
    "    t3.join()\n",
    "    data = queue.get()\n",
    "    logging.info(\"Received response from the API\")\n",
    "    \n",
    "    t4 = threading.Thread(target = create_file, args = (data, timestamps))\n",
    "    t4.start()\n",
    "    t4.join()\n",
    "    logging.info(\"File for received data created\")\n",
    "    \n",
    "    t5 = threading.Thread(target = spark_session, args = (username, queue))\n",
    "    t5.start()\n",
    "    t5.join()\n",
    "    spark = queue.get()\n",
    "    logging.info(\"Spark session created\")\n",
    "    \n",
    "    t6 = threading.Thread(target = mkdir_hdfs, args = (timestamps, hdfs_username))\n",
    "    t6.start()\n",
    "    t6.join()\n",
    "    logging.info(\"HDFS directories created\")\n",
    "    \n",
    "    t7 = threading.Thread(target = create_db, args = (spark,))\n",
    "    t7.start()\n",
    "    t7.join()\n",
    "    logging.info(\"Database creation validated\")\n",
    "    \n",
    "    t8 = threading.Thread(target = copy_files, args = (timestamps, hdfs_username, queue))\n",
    "    t8.start()\n",
    "    t8.join()\n",
    "    file_path = queue.get()\n",
    "    logging.info(\"File copied to HDFS at the location: \", file_path)\n",
    "    \n",
    "    t9 = threading.Thread(target = dummy_thread, args = (timestamps, hdfs_username))\n",
    "    t9.start()\n",
    "    t9.join()\n",
    "    logging.info(\"Checking HDFS-user file directory\")\n",
    "    \n",
    "    t10 = threading.Thread(target = dataframe, args = (hdfs_username, timestamps, spark, file_path, queue))\n",
    "    t10.start()\n",
    "    t10.join()\n",
    "    df = queue.get()\n",
    "    logging.info(\"Dataframe created\")\n",
    "    \n",
    "    t11 = threading.Thread(target = insert_into_table, args = (spark, hdfs_username, df, 'raw'))\n",
    "    t11.start()\n",
    "    t11.join()\n",
    "    logging.info(\"Data inserted into Raw-Table\")\n",
    "    \n",
    "    df_init = df. \\\n",
    "    select('id', 'type','title', 'description', 'location_type',\n",
    "       'location_description', 'media.image_url', 'occurred_at','updated_at', \n",
    "        'type_properties', 'year', 'month', 'day')\n",
    "    \n",
    "    t12 = threading.Thread(target = df_to_json, args = (timestamps, hdfs_username, df_init, 'initial'))\n",
    "    t12.start()\n",
    "    t12.join()\n",
    "    logging.info('Processed JSON file saved')\n",
    "    \n",
    "    t13 = threading.Thread(target = insert_into_table, args = (spark, hdfs_username, df_init, 'initial'))\n",
    "    t13.start()\n",
    "    t13.join()\n",
    "    logging.info(\"Data saved and inserted into Intermediate-DB \")\n",
    "    \n",
    "    logging.info(\"Process completed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd2f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
